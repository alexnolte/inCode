\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Justin Le},
            pdftitle={You Could Have Invented Matrices!},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
% Make links footnotes instead of hotlinks:
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

\title{You Could Have Invented Matrices!}
\author{Justin Le}
\date{March 15, 2018}

\begin{document}
\maketitle

\emph{Originally posted on
\textbf{\href{https://blog.jle.im/entry/you-could-have-invented-matrices.html}{in
Code}}.}

You could have invented matrices!

Let's talk about vectors. A \textbf{vector} (denoted as
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bx\%7D}, a
lower-case bold italicized letter) is an element in a \textbf{vector space},
which means that it can be ``scaled'', like
\includegraphics{https://latex.codecogs.com/png.latex?c\%20\%5Cmathbf\%7Bx\%7D}
(the \includegraphics{https://latex.codecogs.com/png.latex?c} is called a
``scalar'' --- creative name, right?) and added, like
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bx\%7D\%20\%2B\%20\%5Cmathbf\%7By\%7D}.

In order for vector spaces and their operations to be valid, they just have to
obey some
\href{https://en.wikipedia.org/wiki/Vector_space\#Definition}{common-sense
rules} (like associativity, commutativity, distributivity, etc.) that allow us
to make meaningful conclusions.

\hypertarget{dimensionality}{%
\section{Dimensionality}\label{dimensionality}}

One neat thing about vector spaces is that, in \emph{some} of them, you have the
ability to ``decompose'' any vector in it as a weighted sum of some set of
\textbf{basis vectors}. If this is the case for your vector space, then the size
of smallest possible set of basis vectors is known as the \textbf{dimension} of
that vector space.

For example, for a 3-dimensional vector space
\includegraphics{https://latex.codecogs.com/png.latex?V}, any vector
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bx\%7D} can
be described as a weighted sum of three basis vectors. If we call them
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bv\%7D_1},
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bv\%7D_2},
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bv\%7D_3},
then:

{[} \textbackslash mathbf\{x\} = a \textbackslash mathbf\{v\}\_1 + b
\textbackslash mathbf\{v\}\_2 + c
\textbackslash mathbf\{v\}\_3{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cmathbf\%7Bx\%7D\%20\%3D\%20a\%20\%5Cmathbf\%7Bv\%7D\_1\%20\%2B\%20b\%20\%5Cmathbf\%7Bv\%7D\_2\%20\%2B\%20c\%20\%5Cmathbf\%7Bv\%7D\_3\%0A
" \mathbf{x} = a \mathbf{v}\_1 + b \mathbf{v}\_2 + c \mathbf{v}\_3 ")

Where \includegraphics{https://latex.codecogs.com/png.latex?a},
\includegraphics{https://latex.codecogs.com/png.latex?b}, and
\includegraphics{https://latex.codecogs.com/png.latex?c} are scalars of
\includegraphics{https://latex.codecogs.com/png.latex?V}.

Dimensionality is really a statement about being able to \emph{decompose} any
vector in that vector space into a combination of a set of basis vectors. For a
3-dimensional vector space, you can make a bases that can reproduce \emph{any}
vector in your space\ldots but that's only possible with at least three vectors.
a 4-dimensional vector space, you'd need at least four vectors.

For example, in physics, we often treat reality as taking place in a
three-dimensional vector space. The basis vectors are often called
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7B\%5Cmathbf\%7Bi\%7D\%7D},
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7B\%5Cmathbf\%7Bj\%7D\%7D},
and
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7B\%5Cmathbf\%7Bk\%7D\%7D},
and so we say that we can describe our 3D physics vectors as
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Br\%7D\%20\%3D\%20r_x\%20\%5Chat\%7B\%5Cmathbf\%7Bi\%7D\%7D\%20\%2B\%20r_y\%20\%5Chat\%7B\%5Cmathbf\%7Bj\%7D\%7D\%20\%2B\%20r_x\%20\%5Chat\%7B\%5Cmathbf\%7Bk\%7D\%7D}.

\hypertarget{encoding}{%
\subsection{Encoding}\label{encoding}}

One neat thing that physicists take advantage of all the time is that if we
\emph{agree} on a set of basis vectors and a specific ordering, we can actually
\emph{encode} any vector
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bx\%7D} in
terms of those basis vectors.

In physics, for instance, we can say ``Let's encode vectors in terms of sums of
scalings of
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7B\%5Cmathbf\%7Bi\%7D\%7D},
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7B\%5Cmathbf\%7Bj\%7D\%7D},
and
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7B\%5Cmathbf\%7Bk\%7D\%7D},
in that order.'' Then, we can \emph{write}
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Br\%7D} as
\includegraphics{https://latex.codecogs.com/png.latex?\%5Clangle\%20r_x\%2C\%20r_y\%2C\%20r_z\%20\%5Crangle},
and understand that we really mean
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Br\%7D\%20\%3D\%20r_x\%20\%5Chat\%7B\%5Cmathbf\%7Bi\%7D\%7D\%20\%2B\%20r_y\%20\%5Chat\%7B\%5Cmathbf\%7Bj\%7D\%7D\%20\%2B\%20r_x\%20\%5Chat\%7B\%5Cmathbf\%7Bk\%7D\%7D}.

It should be made clear that
\includegraphics{https://latex.codecogs.com/png.latex?\%5Clangle\%20x_1\%2C\%20x_2\%2C\%20x_3\%20\%5Crangle}
is \textbf{not} the same thing as the \emph{vector}
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bx\%7D}. It
is \emph{an encoding} of that vector, which only makes sense once we choose to
\emph{agree} on a specific set of basis. If we chose a different basis, we'd
have a different encoding.

In the general case, for an N-dimensional vector space, this means that, with a
minimum of N items, we can represent any vector in that space. And, if we agree
on those N items, we can devise an encoding, such that:

{[} \textbackslash langle x\_1, x\_2 \textbackslash dots x\_N
\textbackslash rangle{]}(https://latex.codecogs.com/png.latex?\%0A\%5Clangle\%20x\_1\%2C\%20x\_2\%20\%5Cdots\%20x\_N\%20\%5Crangle\%0A
" \langle x\_1, x\_2 \dots x\_N \rangle ")

will \emph{encode} the vector:

{[} x\_1 \textbackslash mathbf\{v\}\_1 + x\_2 \textbackslash mathbf\{v\}\_2 +
\textbackslash ldots + x\_N
\textbackslash mathbf\{v\}\_N{]}(https://latex.codecogs.com/png.latex?\%0Ax\_1\%20\%5Cmathbf\%7Bv\%7D\_1\%20\%2B\%20x\_2\%20\%5Cmathbf\%7Bv\%7D\_2\%20\%2B\%20\%5Cldots\%20\%2B\%20x\_N\%20\%5Cmathbf\%7Bv\%7D\_N\%0A
" x\_1 \mathbf{v}\_1 + x\_2 \mathbf{v}\_2 + \ldots + x\_N \mathbf{v}\_N ")

Note that what this encoding represents is \emph{completely dependent} on what
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bv\%7D_1\%2C\%20\%5Cmathbf\%7Bv\%7D_2\%20\%5Cldots\%20\%5Cmathbf\%7Bv\%7D_N}
we pick, and in what order. The basis vectors we pick are arbitrary, and
determine what our encoding looks like.

To highlight this, note that the same vector
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bx\%7D} has
many different potential encodings --- all you have to do is pick a different
set of basis vectors, or even just re-arrange or re-scale the ones you already
have. However, all of those encodings correspond go the same vector
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bv\%7D}.

One interesting consequence of this is that any N-dimensional vector space whose
scalars are in
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D} is
actually isomorphic to
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D\%5EN}
(the vector space of N-tuples of real numbers). This means that we can basically
treat any N-dimensional vector space with
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D}
scalars as if it was
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D\%5EN},
\emph{once we decide} on the basis vectors. Because of this, we often call
\emph{all} N-dimensional vector spaces (whose scalars are in
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D})
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D\%5EN}.
You will often hear physicists saying that the three-dimensional vector spaces
they use are
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D\%5E3}.
However, what they really mean is that their vector spaces is \emph{isomorphic}
to
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D\%5E3}.

\hypertarget{linear-transformations}{%
\section{Linear Transformations}\label{linear-transformations}}

Now, one of the most interesting things in mathematics is the idea of the
\textbf{linear transformation}. Linear transformations are useful to study
because:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  They are ubiquitous. They come up everywhere in engineering, physics,
  mathematics, data science, economics, and pretty much any mathematical theory.
  And there are even more situations which can be \emph{approximated} by linear
  transformations.
\item
  They are mathematically very nice to work with and study, in practice.
\end{enumerate}

A linear transformation,
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bx\%7D\%29},
is a function that ``respects'' addition and scaling:

{[} \textbackslash begin\{aligned\} f(c\textbackslash mathbf\{x\}) \& = c
f(\textbackslash mathbf\{x\}) \textbackslash\textbackslash{}
f(\textbackslash mathbf\{x\} + \textbackslash mathbf\{y\}) \& =
f(\textbackslash mathbf\{x\}) + f(\textbackslash mathbf\{y\})
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0Af\%28c\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20c\%20f\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bx\%7D\%20\%2B\%20\%5Cmathbf\%7By\%7D\%29\%20\%26\%20\%3D\%20f\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%2B\%20f\%28\%5Cmathbf\%7By\%7D\%29\%0A\%5Cend\%7Baligned\%7D\%0A
"

\begin{aligned}
f(c\mathbf{x}) & = c f(\mathbf{x}) \\
f(\mathbf{x} + \mathbf{y}) & = f(\mathbf{x}) + f(\mathbf{y})
\end{aligned}

")

This means that if you scale the input, the output is scaled by the same amount.
And also, if you transform the sum of two things, it's the same as the sum of
the transformed things (it ``distributes'').

Note that I snuck in vector notation, because the concept of vectors are
\emph{perfectly suited} for studying linear transformations. That's because
talking about linear transformations requires talking about scaling and adding,
and\ldots hey, that's just exactly what vectors have!

From now on, we'll talk about linear transformations specifically on
\emph{N-dimensional vector spaces} (vector spaces that have dimensions and bases
we can use).

\hypertarget{studying-linear-transformations}{%
\subsection{Studying linear
transformations}\label{studying-linear-transformations}}

From first glance, a linear transformation's description doesn't look too useful
or analyzable. All you have is
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bx\%7D\%29}.
It could be anything! Right? Just a black box function?

But, actually, we can exploit its linearity and the fact that we're in a vector
space with a basis to analyze the heck out of any linear transformation, and see
that all of them actually have to follow some specific pattern.

Let's say that
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bx\%7D\%29}
is a linear transformation from N-dimensional vector space
\includegraphics{https://latex.codecogs.com/png.latex?V} to M-dimensional vector
space \includegraphics{https://latex.codecogs.com/png.latex?U}. That is,
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%3A\%20V\%20\%5Crightarrow\%20U}.

Because we know that, once we pick a set of basis vectors
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bv\%7D_i},
any vector
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bx\%7D} in
\includegraphics{https://latex.codecogs.com/png.latex?V} can be decomposed as
\includegraphics{https://latex.codecogs.com/png.latex?x_1\%20\%5Cmathbf\%7Bv\%7D_1\%20\%2B\%20x_2\%20\%5Cmathbf\%7Bv\%7D_2\%20\%2B\%20\%5Cldots\%20x_n\%20\%5Cmathbf\%7Bv\%7D_N},
we really can just look at how a transformation
\includegraphics{https://latex.codecogs.com/png.latex?f} acts on this
decomposition. For example, if
\includegraphics{https://latex.codecogs.com/png.latex?V} is three-dimensional:

{[} f(\textbackslash mathbf\{x\}) = f(x\_1 \textbackslash mathbf\{v\}\_1 + x\_2
\textbackslash mathbf\{v\}\_2 + x\_3
\textbackslash mathbf\{v\}\_3){]}(https://latex.codecogs.com/png.latex?\%0Af\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%3D\%20f\%28x\_1\%20\%5Cmathbf\%7Bv\%7D\_1\%20\%2B\%20x\_2\%20\%5Cmathbf\%7Bv\%7D\_2\%20\%2B\%20x\_3\%20\%5Cmathbf\%7Bv\%7D\_3\%29\%0A
" f(\mathbf{x}) = f(x\_1 \mathbf{v}\_1 + x\_2 \mathbf{v}\_2 + x\_3
\mathbf{v}\_3) ")

Hm. Doesn't seem very insightful, does it?

\hypertarget{a-simple-definition}{%
\subsection{A simple definition}\label{a-simple-definition}}

But! We can exploit the linearity of
\includegraphics{https://latex.codecogs.com/png.latex?f} (that it distributes
and scales) to rewrite that as:

{[} f(\textbackslash mathbf\{x\}) = x\_1 f(\textbackslash mathbf\{v\}\_1) + x\_2
f(\textbackslash mathbf\{v\}\_2) + x\_3
f(\textbackslash mathbf\{v\}\_3){]}(https://latex.codecogs.com/png.latex?\%0Af\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%3D\%20x\_1\%20f\%28\%5Cmathbf\%7Bv\%7D\_1\%29\%20\%2B\%20x\_2\%20f\%28\%5Cmathbf\%7Bv\%7D\_2\%29\%20\%2B\%20x\_3\%20f\%28\%5Cmathbf\%7Bv\%7D\_3\%29\%0A
" f(\mathbf{x}) = x\_1 f(\mathbf{v}\_1) + x\_2 f(\mathbf{v}\_2) + x\_3
f(\mathbf{v}\_3) ")

Okay, take a moment to pause and take that all in. This is actually a pretty big
deal! This just means that, to study
\includegraphics{https://latex.codecogs.com/png.latex?f}, \textbf{all you need
to study} is how \includegraphics{https://latex.codecogs.com/png.latex?f} acts
on our \emph{basis vectors}. If you know how
\includegraphics{https://latex.codecogs.com/png.latex?f} acts on our basis
vectors of our vector space, that's really ``all there is'' about
\includegraphics{https://latex.codecogs.com/png.latex?f}! Not such a black box
anymore!

That is, if I were to ask you, ``Hey, what is
\includegraphics{https://latex.codecogs.com/png.latex?f} like?'', \emph{all
you'd have to tell me} is the result of
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bv\%7D_1\%29},
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bv\%7D_2\%29},
and
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bv\%7D_3\%29}.
Just give me those three \emph{vectors}, and we \emph{uniquely determine
\includegraphics{https://latex.codecogs.com/png.latex?f}}.

To put in another way, \emph{any linear transformation} from a three-dimensional
vector space is uniquely characterized and determined by \emph{three vectors}:
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bv\%7D_1\%29},
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bv\%7D_2\%29},
and
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bv\%7D_3\%29}.
Those three vectors \emph{completely define}
\includegraphics{https://latex.codecogs.com/png.latex?f}.

In general, we see that \emph{any linear transformation} from an N-dimensional
vector space can be \emph{completely defined} by N vectors: the N results of
that transformation on each of N basis vectors we choose.

\hypertarget{enter-the-matrix}{%
\subsection{Enter the Matrix}\label{enter-the-matrix}}

Okay, so how do we ``give''/define/state those N vectors?

Well, recall that the result of
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bx\%7D\%29}
and
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bv\%7D_1\%29},
etc. are \emph{themselves} vectors, in M-dimensional vector space
\includegraphics{https://latex.codecogs.com/png.latex?U}. Let's say that
\includegraphics{https://latex.codecogs.com/png.latex?U} is 2-dimensional, for
now.

This means that any vector
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7By\%7D} in
\includegraphics{https://latex.codecogs.com/png.latex?U} can be represented as
\includegraphics{https://latex.codecogs.com/png.latex?y_1\%20\%5Cmathbf\%7Bu\%7D_1\%20\%2B\%20y_2\%20\%5Cmathbf\%7Bu\%7D_2},
where
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bu\%7D_1} and
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bu\%7D_2} is
an arbitrary choice of basis vectors.

This means that
\includegraphics{https://latex.codecogs.com/png.latex?f\%28\%5Cmathbf\%7Bv\%7D_1\%29}
etc. can also all be represented in terms of these basis vectors. So, laying it
all out:

{[} \textbackslash begin\{aligned\} f(\textbackslash mathbf\{v\}\_1) \& =
a\_\{11\} \textbackslash mathbf\{u\}\_1 + a\_\{21\}
\textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{}
f(\textbackslash mathbf\{v\}\_2) \& = a\_\{12\} \textbackslash mathbf\{u\}\_1 +
a\_\{22\} \textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{}
f(\textbackslash mathbf\{v\}\_3) \& = a\_\{13\} \textbackslash mathbf\{u\}\_1 +
a\_\{23\} \textbackslash mathbf\{u\}\_2
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0Af\%28\%5Cmathbf\%7Bv\%7D\_1\%29\%20\%26\%20\%3D\%20a\_\%7B11\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B21\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bv\%7D\_2\%29\%20\%26\%20\%3D\%20a\_\%7B12\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B22\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bv\%7D\_3\%29\%20\%26\%20\%3D\%20a\_\%7B13\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B23\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} f(\mathbf{v}\emph{1) \& = a}\{11\}
\mathbf{u}\emph{1 + a}\{21\} \mathbf{u}\_2 \textbackslash{} f(\mathbf{v}\emph{2)
\& = a}\{12\} \mathbf{u}\emph{1 + a}\{22\} \mathbf{u}\_2 \textbackslash{}
f(\mathbf{v}\emph{3) \& = a}\{13\} \mathbf{u}\emph{1 + a}\{23\} \mathbf{u}\_2
\textbackslash end\{aligned\} ")

Or, to use our bracket notation from before:

{[} \textbackslash begin\{aligned\} f(\textbackslash mathbf\{v\}\_1) \& =
\textbackslash langle a\_\{11\}, a\_\{21\} \textbackslash rangle
\textbackslash\textbackslash{} f(\textbackslash mathbf\{v\}\_2) \& =
\textbackslash langle a\_\{12\}, a\_\{22\} \textbackslash rangle
\textbackslash\textbackslash{} f(\textbackslash mathbf\{v\}\_3) \& =
\textbackslash langle a\_\{13\}, a\_\{23\} \textbackslash rangle
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0Af\%28\%5Cmathbf\%7Bv\%7D\_1\%29\%20\%26\%20\%3D\%20\%5Clangle\%20a\_\%7B11\%7D\%2C\%20a\_\%7B21\%7D\%20\%5Crangle\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bv\%7D\_2\%29\%20\%26\%20\%3D\%20\%5Clangle\%20a\_\%7B12\%7D\%2C\%20a\_\%7B22\%7D\%20\%5Crangle\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bv\%7D\_3\%29\%20\%26\%20\%3D\%20\%5Clangle\%20a\_\%7B13\%7D\%2C\%20a\_\%7B23\%7D\%20\%5Crangle\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} f(\mathbf{v}\emph{1) \& = \langle a}\{11\},
a\_\{21\} \rangle \textbackslash{} f(\mathbf{v}\emph{2) \& = \langle a}\{12\},
a\_\{22\} \rangle \textbackslash{} f(\mathbf{v}\emph{3) \& = \langle a}\{13\},
a\_\{23\} \rangle \textbackslash end\{aligned\} ")

So, we now see two facts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A linear transformation from an N dimensional vector space to an M dimensional
  vector space can be \emph{defined} using N vectors.
\item
  Each of those N vectors can, themselves, be defined using M scalars each.
\end{enumerate}

Our final conclusion: \emph{any} linear transformation from an N dimensional
vector space to an M dimensional vector space can be completely defined using
\includegraphics{https://latex.codecogs.com/png.latex?N\%20M} scalars.

That's right -- \emph{all} possible linear transformations from a 3-dimensional
vector space to a 2-dimensional are parameterized by only \emph{six} scalars!
These six scalars uniquely determine and define our linear transformation, given
a set of basis vectors that we agree on. All linear transformations
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbb\%7BR\%7D\%5E3\%20\%5Crightarrow\%20\%5Cmathbb\%7BR\%7D\%5E2}
can be defined/encoded/expressed with just six real numbers.

These six numbers are pretty important. Just like how we often talk about
3-dimensional vectors in terms of the encoding of their three coefficients, we
often talk about linear transformations from 3-d space to 2-d space in terms of
their six defining coefficients.

We group these things up in something called a \emph{matrix}.

If our linear transformation
\includegraphics{https://latex.codecogs.com/png.latex?f} from a 3-dimensional
vector space to a 2-dimensional vector space is defined by:

{[} \textbackslash begin\{aligned\} f(\textbackslash mathbf\{v\}\_1) \& =
a\_\{11\} \textbackslash mathbf\{u\}\_1 + a\_\{21\}
\textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{}
f(\textbackslash mathbf\{v\}\_2) \& = a\_\{12\} \textbackslash mathbf\{u\}\_1 +
a\_\{22\} \textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{}
f(\textbackslash mathbf\{v\}\_3) \& = a\_\{13\} \textbackslash mathbf\{u\}\_1 +
a\_\{23\} \textbackslash mathbf\{u\}\_2
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0Af\%28\%5Cmathbf\%7Bv\%7D\_1\%29\%20\%26\%20\%3D\%20a\_\%7B11\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B21\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bv\%7D\_2\%29\%20\%26\%20\%3D\%20a\_\%7B12\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B22\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bv\%7D\_3\%29\%20\%26\%20\%3D\%20a\_\%7B13\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B23\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} f(\mathbf{v}\emph{1) \& = a}\{11\}
\mathbf{u}\emph{1 + a}\{21\} \mathbf{u}\_2 \textbackslash{} f(\mathbf{v}\emph{2)
\& = a}\{12\} \mathbf{u}\emph{1 + a}\{22\} \mathbf{u}\_2 \textbackslash{}
f(\mathbf{v}\emph{3) \& = a}\{13\} \mathbf{u}\emph{1 + a}\{23\} \mathbf{u}\_2
\textbackslash end\{aligned\} ")

(for arbitrary choice of bases
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bv\%7D_i} and
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bu\%7D_i})

We ``encode'' it as the matrix:

{[} f \textbackslash sim \textbackslash begin\{bmatrix\} a\_\{11\} \& a\_\{12\}
\& a\_\{13\} \textbackslash\textbackslash{} a\_\{21\} \& a\_\{22\} \& a\_\{23\}
\textbackslash end\{bmatrix\}{]}(https://latex.codecogs.com/png.latex?\%0Af\%0A\%5Csim\%0A\%5Cbegin\%7Bbmatrix\%7D\%0Aa\_\%7B11\%7D\%20\%26\%20a\_\%7B12\%7D\%20\%26\%20a\_\%7B13\%7D\%20\%5C\%5C\%0Aa\_\%7B21\%7D\%20\%26\%20a\_\%7B22\%7D\%20\%26\%20a\_\%7B23\%7D\%0A\%5Cend\%7Bbmatrix\%7D\%0A
" f \sim \textbackslash begin\{bmatrix\} a\_\{11\} \& a\_\{12\} \& a\_\{13\}
\textbackslash{} a\_\{21\} \& a\_\{22\} \& a\_\{23\}
\textbackslash end\{bmatrix\} ")

And that's why we use matrices in linear algebra -- like how
\includegraphics{https://latex.codecogs.com/png.latex?\%5Clangle\%20x\%2C\%20y\%2C\%20z\%20\%5Crangle}
is a convenient way to represent and define a \emph{vector} (once we agree on a
bases), a
\includegraphics{https://latex.codecogs.com/png.latex?M\%20\%5Ctimes\%20N}
matrix is a convenient way to represent and define a \emph{linear
transformation} from an N-dimensional vector space to a M-dimensional vector
space (once we agree on the bases in both spaces).

\hypertarget{example}{%
\subsection{Example}\label{example}}

Let's look at the vector space of polynomials, which includes vectors like
\includegraphics{https://latex.codecogs.com/png.latex?5\%20p\%5E2\%20-\%203\%20p\%20\%2B\%202},
etc.; scaling a polynomial just means scaling the coefficients, and adding
together polynomials is just normal polynomial addition.

It's an infinite-dimensional vector space, and one popular basis for this vector
space is the polynomials
\includegraphics{https://latex.codecogs.com/png.latex?1\%2C\%20p\%2C\%20p\%5E2\%2C\%20p\%5E3\%2C\%20\%5Cldots},
etc.

With this choice of basis, we can encode a polynomial like
\includegraphics{https://latex.codecogs.com/png.latex?5\%20p\%5E2\%20-\%203\%20p\%20\%2B\%202}
with the notation
\includegraphics{https://latex.codecogs.com/png.latex?\%5Clangle\%202\%2C\%20-3\%2C\%205\%2C\%200\%2C\%200\%2C\%20\%5Cldots\%20\%5Crangle}.

One popular linear transformation on polynomials is the derivative,
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cfrac\%7Bd\%7D\%7Bdp\%7D}.
It takes
\includegraphics{https://latex.codecogs.com/png.latex?5\%20p\%5E2\%20-\%203\%20p\%20\%2B\%202}
and returns
\includegraphics{https://latex.codecogs.com/png.latex?10\%20p\%20-\%203}. In the
basis we just mentioned, it takes
\includegraphics{https://latex.codecogs.com/png.latex?\%5Clangle\%202\%2C\%20-3\%2C\%205\%2C\%200\%2C\%20\%5Cldots\%20\%5Crangle}
and returns
\includegraphics{https://latex.codecogs.com/png.latex?\%5Clangle\%20-3\%2C\%2010\%2C\%200\%2C\%200\%2C\%20\%5Cldots\%20\%5Crangle}.

Now, if you don't know what a derivative was, or how to compute it -- you're in
luck! What we just found out was that if you want to completely understand a
linear transformation, you just need to know \emph{how it works on the basis}!
You just need to know what
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%201\%2C\%20\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%20p\%2C\%20\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%20p\%5E2\%2C\%20\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%20p\%5E3\%2C\%20\%5Cldots}
etc. are, and you basically know everything about what the derivative does.

All I need to do is tell you that
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%20p\%5En\%20\%3D\%20n\%20p\%5E\%7Bn\%20-\%201\%7D}
(what it does to each basis -- the trusty
\href{https://en.wikipedia.org/wiki/Power_rule}{power rule}), and now you know
everything you need to know about derivatives on polynomials. You can basically
just skip all of calculus!

Let's look at what this linear transformation does to each basis:

{[} \textbackslash begin\{aligned\} \textbackslash frac\{d\}\{dp\} 1 \& = 0 \& =
0 + 0 p + 0 p\^{}2 + 0 p\^{}3 + \textbackslash ldots \& = \textbackslash langle
0, 0, 0, 0 \textbackslash ldots \textbackslash rangle
\textbackslash\textbackslash{} \textbackslash frac\{d\}\{dp\} p \& = 1 \& = 1 +
0 p + 0 p\^{}2 + 0 p\^{}3 + \textbackslash ldots \& = \textbackslash langle 1,
0, 0, 0 \textbackslash ldots \textbackslash rangle
\textbackslash\textbackslash{} \textbackslash frac\{d\}\{dp\} p\^{}2 \& = 2 p \&
= 0 + 2 p + 0 p\^{}2 + 0 p\^{}3 + \textbackslash ldots \& =
\textbackslash langle 0, 2, 0, 0 \textbackslash ldots \textbackslash rangle
\textbackslash\textbackslash{} \textbackslash frac\{d\}\{dp\} p\^{}3 \& = 3
p\^{}2 \& = 0 + 0 p + 3 p\^{}2 + 0 p\^{}3 + \textbackslash ldots \& =
\textbackslash langle 0, 0, 3, 0 \textbackslash ldots \textbackslash rangle
\textbackslash\textbackslash{} \textbackslash frac\{d\}\{dp\} p\^{}4 \& = 4
p\^{}3 \& = 0 + 0 p + 0 p\^{}2 + 4 p\^{}3 + \textbackslash ldots \& =
\textbackslash langle 0, 0, 0, 4 \textbackslash ldots \textbackslash rangle
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0A\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%201\%20\%20\%20\%26\%20\%3D\%200\%20\%20\%20\%20\%20\%26\%20\%3D\%200\%20\%2B\%200\%20p\%20\%2B\%200\%20p\%5E2\%20\%2B\%200\%20p\%5E3\%20\%2B\%20\%5Cldots\%0A\%20\%20\%20\%20\%26\%20\%3D\%20\%5Clangle\%200\%2C\%200\%2C\%200\%2C\%200\%20\%5Cldots\%20\%5Crangle\%20\%20\%5C\%5C\%0A\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%20p\%20\%20\%20\%26\%20\%3D\%201\%20\%20\%20\%20\%20\%26\%20\%3D\%201\%20\%2B\%200\%20p\%20\%2B\%200\%20p\%5E2\%20\%2B\%200\%20p\%5E3\%20\%2B\%20\%5Cldots\%0A\%20\%20\%20\%20\%26\%20\%3D\%20\%5Clangle\%201\%2C\%200\%2C\%200\%2C\%200\%20\%5Cldots\%20\%5Crangle\%20\%20\%5C\%5C\%0A\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%20p\%5E2\%20\%26\%20\%3D\%202\%20p\%20\%20\%20\%26\%20\%3D\%200\%20\%2B\%202\%20p\%20\%2B\%200\%20p\%5E2\%20\%2B\%200\%20p\%5E3\%20\%2B\%20\%5Cldots\%0A\%20\%20\%20\%20\%26\%20\%3D\%20\%5Clangle\%200\%2C\%202\%2C\%200\%2C\%200\%20\%5Cldots\%20\%5Crangle\%20\%20\%5C\%5C\%0A\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%20p\%5E3\%20\%26\%20\%3D\%203\%20p\%5E2\%20\%26\%20\%3D\%200\%20\%2B\%200\%20p\%20\%2B\%203\%20p\%5E2\%20\%2B\%200\%20p\%5E3\%20\%2B\%20\%5Cldots\%0A\%20\%20\%20\%20\%26\%20\%3D\%20\%5Clangle\%200\%2C\%200\%2C\%203\%2C\%200\%20\%5Cldots\%20\%5Crangle\%20\%20\%5C\%5C\%0A\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%20p\%5E4\%20\%26\%20\%3D\%204\%20p\%5E3\%20\%26\%20\%3D\%200\%20\%2B\%200\%20p\%20\%2B\%200\%20p\%5E2\%20\%2B\%204\%20p\%5E3\%20\%2B\%20\%5Cldots\%0A\%20\%20\%20\%20\%26\%20\%3D\%20\%5Clangle\%200\%2C\%200\%2C\%200\%2C\%204\%20\%5Cldots\%20\%5Crangle\%0A\%5Cend\%7Baligned\%7D\%0A
"

\begin{aligned}
\frac{d}{dp} 1   & = 0     & = 0 + 0 p + 0 p^2 + 0 p^3 + \ldots
    & = \langle 0, 0, 0, 0 \ldots \rangle  \\
\frac{d}{dp} p   & = 1     & = 1 + 0 p + 0 p^2 + 0 p^3 + \ldots
    & = \langle 1, 0, 0, 0 \ldots \rangle  \\
\frac{d}{dp} p^2 & = 2 p   & = 0 + 2 p + 0 p^2 + 0 p^3 + \ldots
    & = \langle 0, 2, 0, 0 \ldots \rangle  \\
\frac{d}{dp} p^3 & = 3 p^2 & = 0 + 0 p + 3 p^2 + 0 p^3 + \ldots
    & = \langle 0, 0, 3, 0 \ldots \rangle  \\
\frac{d}{dp} p^4 & = 4 p^3 & = 0 + 0 p + 0 p^2 + 4 p^3 + \ldots
    & = \langle 0, 0, 0, 4 \ldots \rangle
\end{aligned}

")

And so, in that
\includegraphics{https://latex.codecogs.com/png.latex?1\%2C\%20p\%2C\%20p\%5E2\%2C\%20p\%5E3\%2C\%20\%5Cldots}
basis, the derivative of a polynomial can be represented as the matrix:

{[} \textbackslash frac\{d\}\{dp\} \textbackslash sim
\textbackslash begin\{bmatrix\} 0 \& 1 \& 0 \& 0 \& 0 \& \textbackslash ldots
\textbackslash\textbackslash{} 0 \& 0 \& 2 \& 0 \& 0 \& \textbackslash ldots
\textbackslash\textbackslash{} 0 \& 0 \& 0 \& 3 \& 0 \& \textbackslash ldots
\textbackslash\textbackslash{} 0 \& 0 \& 0 \& 0 \& 4 \& \textbackslash ldots
\textbackslash\textbackslash{} 0 \& 0 \& 0 \& 0 \& 0 \& \textbackslash ldots
\textbackslash\textbackslash{} \textbackslash vdots \& \textbackslash vdots \&
\textbackslash vdots \& \textbackslash vdots \& \textbackslash vdots \&
\textbackslash ddots
\textbackslash end\{bmatrix\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cfrac\%7Bd\%7D\%7Bdp\%7D\%0A\%5Csim\%0A\%5Cbegin\%7Bbmatrix\%7D\%0A0\%20\%26\%201\%20\%26\%200\%20\%26\%200\%20\%26\%200\%20\%26\%20\%5Cldots\%20\%5C\%5C\%0A0\%20\%26\%200\%20\%26\%202\%20\%26\%200\%20\%26\%200\%20\%26\%20\%5Cldots\%20\%5C\%5C\%0A0\%20\%26\%200\%20\%26\%200\%20\%26\%203\%20\%26\%200\%20\%26\%20\%5Cldots\%20\%5C\%5C\%0A0\%20\%26\%200\%20\%26\%200\%20\%26\%200\%20\%26\%204\%20\%26\%20\%5Cldots\%20\%5C\%5C\%0A0\%20\%26\%200\%20\%26\%200\%20\%26\%200\%20\%26\%200\%20\%26\%20\%5Cldots\%20\%5C\%5C\%0A\%5Cvdots\%20\%26\%20\%5Cvdots\%20\%26\%20\%5Cvdots\%20\%26\%20\%5Cvdots\%20\%26\%20\%5Cvdots\%20\%26\%20\%5Cddots\%0A\%5Cend\%7Bbmatrix\%7D\%0A
" \frac{d}{dp} \sim

\begin{bmatrix}
0 & 1 & 0 & 0 & 0 & \ldots \\
0 & 0 & 2 & 0 & 0 & \ldots \\
0 & 0 & 0 & 3 & 0 & \ldots \\
0 & 0 & 0 & 0 & 4 & \ldots \\
0 & 0 & 0 & 0 & 0 & \ldots \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}

")

No calculus required! (This is the core idea of the
\href{https://en.wikipedia.org/wiki/Formal_derivative}{formal derivative} of a
polynomial)

\hypertarget{matrix-operations}{%
\section{Matrix Operations}\label{matrix-operations}}

In this light, we can understand the definition of the common matrix operations.

\hypertarget{matrix-vector-application}{%
\subsection{Matrix-Vector Application}\label{matrix-vector-application}}

Matrix-vector application (or ``multiplication'') is essentially the
\emph{decoding} of the linear transformation that the matrix represents.

Let's look at the
\includegraphics{https://latex.codecogs.com/png.latex?2\%20\%5Ctimes\%203}
example. Recall that we had:

{[} f(\textbackslash mathbf\{x\}) = x\_1 f(\textbackslash mathbf\{v\}\_1) + x\_2
f(\textbackslash mathbf\{v\}\_2) + x\_3
f(\textbackslash mathbf\{v\}\_3){]}(https://latex.codecogs.com/png.latex?\%0Af\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%3D\%20x\_1\%20f\%28\%5Cmathbf\%7Bv\%7D\_1\%29\%20\%2B\%20x\_2\%20f\%28\%5Cmathbf\%7Bv\%7D\_2\%29\%20\%2B\%20x\_3\%20f\%28\%5Cmathbf\%7Bv\%7D\_3\%29\%0A
" f(\mathbf{x}) = x\_1 f(\mathbf{v}\_1) + x\_2 f(\mathbf{v}\_2) + x\_3
f(\mathbf{v}\_3) ")

And we say that \includegraphics{https://latex.codecogs.com/png.latex?f} is
completely defined by:

{[} \textbackslash begin\{aligned\} f(\textbackslash mathbf\{v\}\_1) \& =
a\_\{11\} \textbackslash mathbf\{u\}\_1 + a\_\{21\}
\textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{}
f(\textbackslash mathbf\{v\}\_2) \& = a\_\{12\} \textbackslash mathbf\{u\}\_1 +
a\_\{22\} \textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{}
f(\textbackslash mathbf\{v\}\_3) \& = a\_\{13\} \textbackslash mathbf\{u\}\_1 +
a\_\{23\} \textbackslash mathbf\{u\}\_2
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0Af\%28\%5Cmathbf\%7Bv\%7D\_1\%29\%20\%26\%20\%3D\%20a\_\%7B11\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B21\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bv\%7D\_2\%29\%20\%26\%20\%3D\%20a\_\%7B12\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B22\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0Af\%28\%5Cmathbf\%7Bv\%7D\_3\%29\%20\%26\%20\%3D\%20a\_\%7B13\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B23\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} f(\mathbf{v}\emph{1) \& = a}\{11\}
\mathbf{u}\emph{1 + a}\{21\} \mathbf{u}\_2 \textbackslash{} f(\mathbf{v}\emph{2)
\& = a}\{12\} \mathbf{u}\emph{1 + a}\{22\} \mathbf{u}\_2 \textbackslash{}
f(\mathbf{v}\emph{3) \& = a}\{13\} \mathbf{u}\emph{1 + a}\{23\} \mathbf{u}\_2
\textbackslash end\{aligned\} ")

This means that:

{[} \textbackslash begin\{aligned\} f(\textbackslash mathbf\{x\}) \& = x\_1
(a\_\{11\} \textbackslash mathbf\{u\}\_1 + a\_\{21\}
\textbackslash mathbf\{u\}\_2) \textbackslash\textbackslash{} \& + x\_2
(a\_\{12\} \textbackslash mathbf\{u\}\_1 + a\_\{22\}
\textbackslash mathbf\{u\}\_2) \textbackslash\textbackslash{} \& + x\_3
(a\_\{13\} \textbackslash mathbf\{u\}\_1 + a\_\{23\}
\textbackslash mathbf\{u\}\_2)
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0Af\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20x\_1\%20\%28a\_\%7B11\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B21\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%29\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20x\_2\%20\%28a\_\%7B12\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B22\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%29\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20x\_3\%20\%28a\_\%7B13\%7D\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%2B\%20a\_\%7B23\%7D\%20\%5Cmathbf\%7Bu\%7D\_2\%29\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} f(\mathbf{x}) \& = x\_1 (a\_\{11\}
\mathbf{u}\emph{1 + a}\{21\} \mathbf{u}\emph{2) \textbackslash{} \& + x\_2
(a}\{12\} \mathbf{u}\emph{1 + a}\{22\} \mathbf{u}\emph{2) \textbackslash{} \& +
x\_3 (a}\{13\} \mathbf{u}\emph{1 + a}\{23\} \mathbf{u}\_2)
\textbackslash end\{aligned\} ")

Which is itself a vector in
\includegraphics{https://latex.codecogs.com/png.latex?U}, so let's write this as
a combination of its components
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bu\%7D_1} and
\includegraphics{https://latex.codecogs.com/png.latex?\%5Cmathbf\%7Bu\%7D_2}, by
distributing and rearranging terms:

{[} \textbackslash begin\{aligned\} f(\textbackslash mathbf\{v\}) \& = (v\_1
a\_\{11\} + v\_2 a\_\{12\} + v\_3 a\_\{13\}) \textbackslash mathbf\{u\}\_1
\textbackslash\textbackslash{} \& + (v\_1 a\_\{21\} + v\_2 a\_\{22\} + v\_3
a\_\{23\}) \textbackslash mathbf\{u\}\_2
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0Af\%28\%5Cmathbf\%7Bv\%7D\%29\%20\%26\%20\%3D\%20\%28v\_1\%20a\_\%7B11\%7D\%20\%2B\%20v\_2\%20a\_\%7B12\%7D\%20\%2B\%20v\_3\%20a\_\%7B13\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20\%28v\_1\%20a\_\%7B21\%7D\%20\%2B\%20v\_2\%20a\_\%7B22\%7D\%20\%2B\%20v\_3\%20a\_\%7B23\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_2\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} f(\mathbf{v}) \& = (v\_1 a\_\{11\} + v\_2
a\_\{12\} + v\_3 a\_\{13\}) \mathbf{u}\emph{1 \textbackslash{} \& + (v\_1
a}\{21\} + v\_2 a\_\{22\} + v\_3 a\_\{23\}) \mathbf{u}\_2
\textbackslash end\{aligned\} ")

And this is exactly the formula for matrix-vector multiplication!

{[} \textbackslash begin\{bmatrix\} a\_\{11\} \& a\_\{12\} \& a\_\{13\}
\textbackslash\textbackslash{} a\_\{21\} \& a\_\{22\} \& a\_\{23\}
\textbackslash end\{bmatrix\} \textbackslash begin\{bmatrix\} x\_1
\textbackslash\textbackslash{} x\_2 \textbackslash\textbackslash{} x\_3
\textbackslash end\{bmatrix\} = \textbackslash begin\{bmatrix\} x\_1 a\_\{11\} +
x\_2 a\_\{12\} + x\_3 a\_\{13\} \textbackslash\textbackslash{} x\_1 a\_\{21\} +
x\_2 a\_\{22\} + x\_3 a\_\{23\}
\textbackslash end\{bmatrix\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Bbmatrix\%7D\%0Aa\_\%7B11\%7D\%20\%26\%20a\_\%7B12\%7D\%20\%26\%20a\_\%7B13\%7D\%20\%5C\%5C\%0Aa\_\%7B21\%7D\%20\%26\%20a\_\%7B22\%7D\%20\%26\%20a\_\%7B23\%7D\%0A\%5Cend\%7Bbmatrix\%7D\%0A\%5Cbegin\%7Bbmatrix\%7D\%0Ax\_1\%20\%5C\%5C\%0Ax\_2\%20\%5C\%5C\%0Ax\_3\%0A\%5Cend\%7Bbmatrix\%7D\%0A\%3D\%0A\%5Cbegin\%7Bbmatrix\%7D\%0Ax\_1\%20a\_\%7B11\%7D\%20\%2B\%20x\_2\%20a\_\%7B12\%7D\%20\%2B\%20x\_3\%20a\_\%7B13\%7D\%20\%5C\%5C\%0Ax\_1\%20a\_\%7B21\%7D\%20\%2B\%20x\_2\%20a\_\%7B22\%7D\%20\%2B\%20x\_3\%20a\_\%7B23\%7D\%0A\%5Cend\%7Bbmatrix\%7D\%0A
" \textbackslash begin\{bmatrix\} a\_\{11\} \& a\_\{12\} \& a\_\{13\}
\textbackslash{} a\_\{21\} \& a\_\{22\} \& a\_\{23\}
\textbackslash end\{bmatrix\} \textbackslash begin\{bmatrix\} x\_1
\textbackslash{} x\_2 \textbackslash{} x\_3 \textbackslash end\{bmatrix\} =
\textbackslash begin\{bmatrix\} x\_1 a\_\{11\} + x\_2 a\_\{12\} + x\_3 a\_\{13\}
\textbackslash{} x\_1 a\_\{21\} + x\_2 a\_\{22\} + x\_3 a\_\{23\}
\textbackslash end\{bmatrix\} ")

Again, remember that what we are doing is manipulating \emph{specific encodings}
of our vectors and our linear transformations. Namely, we encode linear
transformations as matrices, and vectors in their component encoding. The reason
we can do these is that we agree upon a set of bases for our source and target
vector spaces, and express these encodings in terms of those.

The magic we get out of this is that we can manipulate things in our ``encoding
world'', which correspond to things in the ``real world''.

\hypertarget{addition-of-linear-transformations}{%
\subsection{Addition of linear
transformations}\label{addition-of-linear-transformations}}

One neat thing about linear transformation is that they ``add'' well -- you can
add them together by simply applying them both and adding the results. The
result is another linear transformation.

{[} (f + g)(\textbackslash mathbf\{x\}) \textbackslash equiv
f(\textbackslash mathbf\{x\}) +
g(\textbackslash mathbf\{x\}){]}(https://latex.codecogs.com/png.latex?\%0A\%28f\%20\%2B\%20g\%29\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%5Cequiv\%20f\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%2B\%20g\%28\%5Cmathbf\%7Bx\%7D\%29\%0A
" (f + g)(\mathbf{x}) \equiv f(\mathbf{x}) + g(\mathbf{x}) ")

If
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%3A\%20V\%20\%5Crightarrow\%20U}
and
\includegraphics{https://latex.codecogs.com/png.latex?g\%20\%3A\%20V\%20\%5Crightarrow\%20U}
are linear transformations between the \emph{same} vector spaces, then
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%2B\%20g\%20\%3A\%20V\%20\%5Crightarrow\%20U},
as we defined it, is also one:

{[} \textbackslash begin\{aligned\} (f + g)(c \textbackslash mathbf\{x\}) \& =
f(c \textbackslash mathbf\{x\}) + g(c \textbackslash mathbf\{x\})
\textbackslash\textbackslash{} \& = c f(\textbackslash mathbf\{x\}) + c
g(\textbackslash mathbf\{x\}) \textbackslash\textbackslash{} \& = c (
f(\textbackslash mathbf\{x\}) + g(\textbackslash mathbf\{x\}) )
\textbackslash\textbackslash{} (f + g)(c \textbackslash mathbf\{x\}) \& = c (f +
g)(\textbackslash mathbf\{x\})
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0A\%28f\%20\%2B\%20g\%29\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20f\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%2B\%20g\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%3D\%20c\%20f\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%2B\%20c\%20g\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%3D\%20c\%20\%28\%20f\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%2B\%20g\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%29\%20\%5C\%5C\%0A\%28f\%20\%2B\%20g\%29\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20c\%20\%28f\%20\%2B\%20g\%29\%28\%5Cmathbf\%7Bx\%7D\%29\%0A\%5Cend\%7Baligned\%7D\%0A
"

\begin{aligned}
(f + g)(c \mathbf{x}) & = f(c \mathbf{x}) + g(c \mathbf{x}) \\
                      & = c f(\mathbf{x}) + c g(\mathbf{x}) \\
                      & = c ( f(\mathbf{x}) + g(\mathbf{x}) ) \\
(f + g)(c \mathbf{x}) & = c (f + g)(\mathbf{x})
\end{aligned}

")

(Showing that it respects addition is something you can look at if you want to
have some fun!)

So, if \includegraphics{https://latex.codecogs.com/png.latex?f} is encoded as
matrix \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D}
for given bases, and \includegraphics{https://latex.codecogs.com/png.latex?g} is
encoded as matrix
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BB\%7D}, what is
the encoding of
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%2B\%20g} ?

Let's say that, if \includegraphics{https://latex.codecogs.com/png.latex?V} and
\includegraphics{https://latex.codecogs.com/png.latex?U} are 3-dimensional and
2-dimensional, respectively:

{[} \textbackslash begin\{aligned\} f(\textbackslash mathbf\{x\}) \& = (x\_1
a\_\{11\} + x\_2 a\_\{12\} + x\_3 a\_\{13\}) \textbackslash mathbf\{u\}\_1
\textbackslash\textbackslash{} \& + (x\_1 a\_\{21\} + x\_2 a\_\{22\} + x\_3
a\_\{23\}) \textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{}
g(\textbackslash mathbf\{x\}) \& = (x\_1 b\_\{11\} + x\_2 b\_\{12\} + x\_3
b\_\{13\}) \textbackslash mathbf\{u\}\_1 \textbackslash\textbackslash{} \& +
(x\_1 b\_\{21\} + x\_2 b\_\{22\} + x\_3 b\_\{23\}) \textbackslash mathbf\{u\}\_2
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0Af\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20\%28x\_1\%20a\_\%7B11\%7D\%20\%2B\%20x\_2\%20a\_\%7B12\%7D\%20\%2B\%20x\_3\%20a\_\%7B13\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20\%28x\_1\%20a\_\%7B21\%7D\%20\%2B\%20x\_2\%20a\_\%7B22\%7D\%20\%2B\%20x\_3\%20a\_\%7B23\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0Ag\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20\%28x\_1\%20b\_\%7B11\%7D\%20\%2B\%20x\_2\%20b\_\%7B12\%7D\%20\%2B\%20x\_3\%20b\_\%7B13\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20\%28x\_1\%20b\_\%7B21\%7D\%20\%2B\%20x\_2\%20b\_\%7B22\%7D\%20\%2B\%20x\_3\%20b\_\%7B23\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_2\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} f(\mathbf{x}) \& = (x\_1 a\_\{11\} + x\_2
a\_\{12\} + x\_3 a\_\{13\}) \mathbf{u}\emph{1 \textbackslash{} \& + (x\_1
a}\{21\} + x\_2 a\_\{22\} + x\_3 a\_\{23\}) \mathbf{u}\emph{2 \textbackslash{}
g(\mathbf{x}) \& = (x\_1 b}\{11\} + x\_2 b\_\{12\} + x\_3 b\_\{13\})
\mathbf{u}\emph{1 \textbackslash{} \& + (x\_1 b}\{21\} + x\_2 b\_\{22\} + x\_3
b\_\{23\}) \mathbf{u}\_2 \textbackslash end\{aligned\} ")

Then the breakdown of
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%2B\%20g} is:

{[} \textbackslash begin\{aligned\} (f + g)(\textbackslash mathbf\{v\}) \& =
(x\_1 a\_\{11\} + x\_2 a\_\{12\} + x\_3 a\_\{13\}) \textbackslash mathbf\{u\}\_1
\textbackslash\textbackslash{} \& + (x\_1 a\_\{21\} + x\_2 a\_\{22\} + x\_3
a\_\{23\}) \textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{} \& +
(x\_1 b\_\{11\} + x\_2 b\_\{12\} + x\_3 b\_\{13\}) \textbackslash mathbf\{u\}\_1
\textbackslash\textbackslash{} \& + (x\_1 b\_\{21\} + x\_2 b\_\{22\} + x\_3
b\_\{23\}) \textbackslash mathbf\{u\}\_2 \textbackslash\textbackslash{} (f +
g)(\textbackslash mathbf\{v\}) \& = (x\_1 {[}a\_\{11\} + b\_\{11\}{]} + x\_2
{[}a\_\{12\} + b\_\{12\}{]} + x\_3 {[}a\_\{13\} + b\_\{13\}{]})
\textbackslash mathbf\{u\}\_1 \textbackslash\textbackslash{} \& + (x\_1
{[}a\_\{21\} + b\_\{21\}{]} + x\_2 {[}a\_\{22\} + b\_\{22\}{]} + x\_3
{[}a\_\{23\} + b\_\{23\}{]}) \textbackslash mathbf\{u\}\_2
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0A\%28f\%20\%2B\%20g\%29\%28\%5Cmathbf\%7Bv\%7D\%29\%20\%26\%20\%3D\%20\%28x\_1\%20a\_\%7B11\%7D\%20\%2B\%20x\_2\%20a\_\%7B12\%7D\%20\%2B\%20x\_3\%20a\_\%7B13\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20\%28x\_1\%20a\_\%7B21\%7D\%20\%2B\%20x\_2\%20a\_\%7B22\%7D\%20\%2B\%20x\_3\%20a\_\%7B23\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20\%28x\_1\%20b\_\%7B11\%7D\%20\%2B\%20x\_2\%20b\_\%7B12\%7D\%20\%2B\%20x\_3\%20b\_\%7B13\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20\%28x\_1\%20b\_\%7B21\%7D\%20\%2B\%20x\_2\%20b\_\%7B22\%7D\%20\%2B\%20x\_3\%20b\_\%7B23\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_2\%20\%5C\%5C\%0A\%28f\%20\%2B\%20g\%29\%28\%5Cmathbf\%7Bv\%7D\%29\%20\%26\%20\%3D\%20\%28x\_1\%20\%5Ba\_\%7B11\%7D\%20\%2B\%20b\_\%7B11\%7D\%5D\%20\%2B\%20x\_2\%20\%5Ba\_\%7B12\%7D\%20\%2B\%20b\_\%7B12\%7D\%5D\%20\%2B\%20x\_3\%20\%5Ba\_\%7B13\%7D\%20\%2B\%20b\_\%7B13\%7D\%5D\%29\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20\%28x\_1\%20\%5Ba\_\%7B21\%7D\%20\%2B\%20b\_\%7B21\%7D\%5D\%20\%2B\%20x\_2\%20\%5Ba\_\%7B22\%7D\%20\%2B\%20b\_\%7B22\%7D\%5D\%20\%2B\%20x\_3\%20\%5Ba\_\%7B23\%7D\%20\%2B\%20b\_\%7B23\%7D\%5D\%29\%20\%5Cmathbf\%7Bu\%7D\_2\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} (f + g)(\mathbf{v}) \& = (x\_1 a\_\{11\} +
x\_2 a\_\{12\} + x\_3 a\_\{13\}) \mathbf{u}\emph{1 \textbackslash{} \& + (x\_1
a}\{21\} + x\_2 a\_\{22\} + x\_3 a\_\{23\}) \mathbf{u}\emph{2 \textbackslash{}
\& + (x\_1 b}\{11\} + x\_2 b\_\{12\} + x\_3 b\_\{13\}) \mathbf{u}\emph{1
\textbackslash{} \& + (x\_1 b}\{21\} + x\_2 b\_\{22\} + x\_3 b\_\{23\})
\mathbf{u}\_2 \textbackslash{} (f + g)(\mathbf{v}) \& = (x\_1 {[}a\_\{11\} +
b\_\{11\}{]} + x\_2 {[}a\_\{12\} + b\_\{12\}{]} + x\_3 {[}a\_\{13\} +
b\_\{13\}{]}) \mathbf{u}\_1 \textbackslash{} \& + (x\_1 {[}a\_\{21\} +
b\_\{21\}{]} + x\_2 {[}a\_\{22\} + b\_\{22\}{]} + x\_3 {[}a\_\{23\} +
b\_\{23\}{]}) \mathbf{u}\_2 \textbackslash end\{aligned\} ")

Note that if we say that
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%2B\%20g} is encoded
as matrix
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BC\%7D}, and
call the components
\includegraphics{https://latex.codecogs.com/png.latex?c_\%7B11\%7D},
\includegraphics{https://latex.codecogs.com/png.latex?c_\%7B12\%7D}, etc., then
we can rewrite that as:

{[} \textbackslash begin\{aligned\} (f + g)(\textbackslash mathbf\{x\}) \& =
(x\_1 c\_\{11\} + x\_2 c\_\{12\} + x\_3 c\_\{13\}) \textbackslash mathbf\{u\}\_1
\textbackslash\textbackslash{} \& + (x\_1 c\_\{21\} + x\_2 c\_\{22\} + x\_3
c\_\{23\}) \textbackslash mathbf\{u\}\_2
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0A\%28f\%20\%2B\%20g\%29\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20\%28x\_1\%20c\_\%7B11\%7D\%20\%2B\%20x\_2\%20c\_\%7B12\%7D\%20\%2B\%20x\_3\%20c\_\%7B13\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_1\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%2B\%20\%28x\_1\%20c\_\%7B21\%7D\%20\%2B\%20x\_2\%20c\_\%7B22\%7D\%20\%2B\%20x\_3\%20c\_\%7B23\%7D\%29\%20\%5Cmathbf\%7Bu\%7D\_2\%0A\%5Cend\%7Baligned\%7D\%0A
" \textbackslash begin\{aligned\} (f + g)(\mathbf{x}) \& = (x\_1 c\_\{11\} +
x\_2 c\_\{12\} + x\_3 c\_\{13\}) \mathbf{u}\emph{1 \textbackslash{} \& + (x\_1
c}\{21\} + x\_2 c\_\{22\} + x\_3 c\_\{23\}) \mathbf{u}\_2
\textbackslash end\{aligned\} ")

Where
\includegraphics{https://latex.codecogs.com/png.latex?c_\%7B11\%7D\%20\%3D\%20a_\%7B11\%7D\%20\%2B\%20b_\%7B11\%7D},
\includegraphics{https://latex.codecogs.com/png.latex?c_\%7B12\%7D\%20\%3D\%20a_\%7B12\%7D\%20\%2B\%20b_\%7B12\%7D},
etc.

So, if \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D}
and \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BB\%7D}
encode linear transformations
\includegraphics{https://latex.codecogs.com/png.latex?f} and
\includegraphics{https://latex.codecogs.com/png.latex?g}, then we can encode
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%2B\%20g} as matrix
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BC\%7D}, where
the components of
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BC\%7D} are just
the sum of their corresponding components in
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D} and
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BB\%7D}.

And that's why we define
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D\%20\%2B\%20\%5Chat\%7BB\%7D},
matrix-matrix addition, as component-wise addition: component-wise addition
perfectly ``simulates'' the addition of the linear transformation!

What's happening here is we can represent manipulations of the functions
themselves by manipulating \emph{their encodings}.

And, again, the magic here is that, by manipulating things in our ``encoding
world'', we can make meaningful manipulations in the ``real world'' of linear
transformations.

{[} \textbackslash begin\{bmatrix\} a\_\{11\} \& a\_\{12\} \& a\_\{13\}
\textbackslash\textbackslash{} a\_\{21\} \& a\_\{22\} \& a\_\{23\}
\textbackslash end\{bmatrix\} + \textbackslash begin\{bmatrix\} b\_\{11\} \&
b\_\{12\} \& b\_\{13\} \textbackslash\textbackslash{} b\_\{21\} \& b\_\{22\} \&
b\_\{23\} \textbackslash end\{bmatrix\} = \textbackslash begin\{bmatrix\}
a\_\{11\}+b\_\{11\} \& a\_\{12\}+b\_\{12\} \& a\_\{13\}+b\_\{13\}
\textbackslash\textbackslash{} a\_\{21\}+b\_\{21\} \& a\_\{22\}+b\_\{22\} \&
a\_\{23\}+b\_\{23\}
\textbackslash end\{bmatrix\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Bbmatrix\%7D\%0Aa\_\%7B11\%7D\%20\%26\%20a\_\%7B12\%7D\%20\%26\%20a\_\%7B13\%7D\%20\%5C\%5C\%0Aa\_\%7B21\%7D\%20\%26\%20a\_\%7B22\%7D\%20\%26\%20a\_\%7B23\%7D\%0A\%5Cend\%7Bbmatrix\%7D\%0A\%2B\%0A\%5Cbegin\%7Bbmatrix\%7D\%0Ab\_\%7B11\%7D\%20\%26\%20b\_\%7B12\%7D\%20\%26\%20b\_\%7B13\%7D\%20\%5C\%5C\%0Ab\_\%7B21\%7D\%20\%26\%20b\_\%7B22\%7D\%20\%26\%20b\_\%7B23\%7D\%0A\%5Cend\%7Bbmatrix\%7D\%0A\%3D\%0A\%5Cbegin\%7Bbmatrix\%7D\%0Aa\_\%7B11\%7D\%2Bb\_\%7B11\%7D\%20\%26\%20a\_\%7B12\%7D\%2Bb\_\%7B12\%7D\%20\%26\%20a\_\%7B13\%7D\%2Bb\_\%7B13\%7D\%20\%5C\%5C\%0Aa\_\%7B21\%7D\%2Bb\_\%7B21\%7D\%20\%26\%20a\_\%7B22\%7D\%2Bb\_\%7B22\%7D\%20\%26\%20a\_\%7B23\%7D\%2Bb\_\%7B23\%7D\%0A\%5Cend\%7Bbmatrix\%7D\%0A
" \textbackslash begin\{bmatrix\} a\_\{11\} \& a\_\{12\} \& a\_\{13\}
\textbackslash{} a\_\{21\} \& a\_\{22\} \& a\_\{23\}
\textbackslash end\{bmatrix\} + \textbackslash begin\{bmatrix\} b\_\{11\} \&
b\_\{12\} \& b\_\{13\} \textbackslash{} b\_\{21\} \& b\_\{22\} \& b\_\{23\}
\textbackslash end\{bmatrix\} = \textbackslash begin\{bmatrix\}
a\_\{11\}+b\_\{11\} \& a\_\{12\}+b\_\{12\} \& a\_\{13\}+b\_\{13\}
\textbackslash{} a\_\{21\}+b\_\{21\} \& a\_\{22\}+b\_\{22\} \&
a\_\{23\}+b\_\{23\} \textbackslash end\{bmatrix\} ")

Symbolically, if we write function application as matrix-vector multiplication,
we say that
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D\%20\%2B\%20\%5Chat\%7BB\%7D}
is defined so that
\includegraphics{https://latex.codecogs.com/png.latex?\%28\%5Chat\%7BA\%7D\%20\%2B\%20\%5Chat\%7BB\%7D\%29\%5Cmathbf\%7Bx\%7D\%20\%3D\%20\%5Chat\%7BA\%7D\%20\%5Cmathbf\%7Bx\%7D\%20\%2B\%20\%5Chat\%7BB\%7D\%20\%5Cmathbf\%7Bx\%7D}.

\hypertarget{multiplication-of-linear-transformations}{%
\subsection{Multiplication of linear
transformations}\label{multiplication-of-linear-transformations}}

We might be tempted to define \emph{multiplication} of linear transformations
the same way. However, this doesn't quite make sense.

Remember that we talked about adding linear transformations as the addition of
their results. However, we can't talk about multiplying linear transformations
as the multiplication of their results because the idea of a vector space
doesn't come with any notion of multiplication.

However, even if we talk specifically about linear transformations to
\emph{scalars}, this still doesn't quite work:

{[} \textbackslash begin\{aligned\} (f * g)(c \textbackslash mathbf\{x\}) \& =
f(c \textbackslash mathbf\{x\}) g(c \textbackslash mathbf\{x\})
\textbackslash\textbackslash{} \& = c f(\textbackslash mathbf\{x\}) c
g(\textbackslash mathbf\{x\}) \textbackslash\textbackslash{} \& = c\^{}2 (
f(\textbackslash mathbf\{x\}) g(\textbackslash mathbf\{x\}) )
\textbackslash\textbackslash{} (f * g)(c \textbackslash mathbf\{x\}) \& = c\^{}2
(f * g)(\textbackslash mathbf\{x\})
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0A\%28f\%20\%2A\%20g\%29\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20f\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20g\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%3D\%20c\%20f\%28\%5Cmathbf\%7Bx\%7D\%29\%20c\%20g\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%3D\%20c\%5E2\%20\%28\%20f\%28\%5Cmathbf\%7Bx\%7D\%29\%20g\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%29\%20\%5C\%5C\%0A\%28f\%20\%2A\%20g\%29\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20c\%5E2\%20\%28f\%20\%2A\%20g\%29\%28\%5Cmathbf\%7Bx\%7D\%29\%0A\%5Cend\%7Baligned\%7D\%0A
"

\begin{aligned}
(f * g)(c \mathbf{x}) & = f(c \mathbf{x}) g(c \mathbf{x}) \\
                      & = c f(\mathbf{x}) c g(\mathbf{x}) \\
                      & = c^2 ( f(\mathbf{x}) g(\mathbf{x}) ) \\
(f * g)(c \mathbf{x}) & = c^2 (f * g)(\mathbf{x})
\end{aligned}

")

So,
\includegraphics{https://latex.codecogs.com/png.latex?\%28f\%20\%2A\%20g\%29\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%3D\%20c\%5E2\%20\%28f\%20\%2A\%20g\%29\%28\%5Cmathbf\%7Bx\%7D\%29}.
Therefore,
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%2A\%20g}, defined
point-wise, does \emph{not} yield a linear transformation.

Therefore, \emph{there is no matrix} that could would even represent or encode
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%2A\%20g}, as we
defined it. So, since
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%2A\%20g} isn't even
representable as a matrix in our encoding scheme, it doesn't make sense to treat
it as a matrix operation. There's no possible result!

\hypertarget{composition-of-linear-transformations}{%
\subsection{Composition of linear
transformations}\label{composition-of-linear-transformations}}

Since linear transformations are functions, we can compose them:

{[} (f \textbackslash circ g)(\textbackslash mathbf\{x\}) \textbackslash equiv
f(g(\textbackslash mathbf\{x\})){]}(https://latex.codecogs.com/png.latex?\%0A\%28f\%20\%5Ccirc\%20g\%29\%28\%5Cmathbf\%7Bx\%7D\%29\%20\%5Cequiv\%20f\%28g\%28\%5Cmathbf\%7Bx\%7D\%29\%29\%0A
" (f \circ g)(\mathbf{x}) \equiv f(g(\mathbf{x})) ")

Is the composition of linear transformations also a linear transformation?

{[} \textbackslash begin\{aligned\} (f \textbackslash circ g)(c
\textbackslash mathbf\{x\}) \& = f(g(c \textbackslash mathbf\{x\}))
\textbackslash\textbackslash{} \& = f(c g(\textbackslash mathbf\{x\}))
\textbackslash\textbackslash{} \& = c f(g(\textbackslash mathbf\{x\}))
\textbackslash\textbackslash{} (f \textbackslash circ g)(c
\textbackslash mathbf\{x\}) \& = c (f \textbackslash circ
g)(\textbackslash mathbf\{x\})
\textbackslash end\{aligned\}{]}(https://latex.codecogs.com/png.latex?\%0A\%5Cbegin\%7Baligned\%7D\%0A\%28f\%20\%5Ccirc\%20g\%29\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20f\%28g\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%29\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%3D\%20f\%28c\%20g\%28\%5Cmathbf\%7Bx\%7D\%29\%29\%20\%5C\%5C\%0A\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%20\%26\%20\%3D\%20c\%20f\%28g\%28\%5Cmathbf\%7Bx\%7D\%29\%29\%20\%5C\%5C\%0A\%28f\%20\%5Ccirc\%20g\%29\%28c\%20\%5Cmathbf\%7Bx\%7D\%29\%20\%26\%20\%3D\%20c\%20\%28f\%20\%5Ccirc\%20g\%29\%28\%5Cmathbf\%7Bx\%7D\%29\%0A\%5Cend\%7Baligned\%7D\%0A
"

\begin{aligned}
(f \circ g)(c \mathbf{x}) & = f(g(c \mathbf{x})) \\
                      & = f(c g(\mathbf{x})) \\
                      & = c f(g(\mathbf{x})) \\
(f \circ g)(c \mathbf{x}) & = c (f \circ g)(\mathbf{x})
\end{aligned}

")

Yes! (Well, once you prove that it respects addition. I'll leave the fun to
you!)

Okay, so we know that
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%5Ccirc\%20g} is
indeed a linear transformation. That means that it can \emph{also} be encoded as
a matrix.

So, let's say that
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%3A\%20U\%20\%5Crightarrow\%20W},
then
\includegraphics{https://latex.codecogs.com/png.latex?g\%20\%3A\%20V\%20\%5Crightarrow\%20U}.
\includegraphics{https://latex.codecogs.com/png.latex?f} is a linear
transformation from \includegraphics{https://latex.codecogs.com/png.latex?U} to
\includegraphics{https://latex.codecogs.com/png.latex?W}, and
\includegraphics{https://latex.codecogs.com/png.latex?g} is a linear
transformation from \includegraphics{https://latex.codecogs.com/png.latex?V} to
\includegraphics{https://latex.codecogs.com/png.latex?U}. That means that
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%5Ccirc\%20g\%20\%3A\%20V\%20\%5Crightarrow\%20W}
is a linear transformation from
\includegraphics{https://latex.codecogs.com/png.latex?V} to
\includegraphics{https://latex.codecogs.com/png.latex?W}. It goes from
\includegraphics{https://latex.codecogs.com/png.latex?V}, through
\includegraphics{https://latex.codecogs.com/png.latex?U}, and all the way to
\includegraphics{https://latex.codecogs.com/png.latex?W}.

Let's say that \includegraphics{https://latex.codecogs.com/png.latex?V} is
3-dimensional, \includegraphics{https://latex.codecogs.com/png.latex?U} is
2-dimensional, and \includegraphics{https://latex.codecogs.com/png.latex?W} is
4-dimensional.

If \includegraphics{https://latex.codecogs.com/png.latex?f} is encoded by the
\includegraphics{https://latex.codecogs.com/png.latex?4\%20\%5Ctimes\%202}
matrix \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D},
and \includegraphics{https://latex.codecogs.com/png.latex?g} is encoded by
\includegraphics{https://latex.codecogs.com/png.latex?2\%20\%5Ctimes\%203}
matrix \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BB\%7D},
then we can represent
\includegraphics{https://latex.codecogs.com/png.latex?f\%20\%5Ccirc\%20g} as the
\includegraphics{https://latex.codecogs.com/png.latex?4\%20\%5Ctimes\%203}
matrix \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BC\%7D}.

If you've taken a linear algebra class, you might recognize this pattern.
Combining a
\includegraphics{https://latex.codecogs.com/png.latex?4\%20\%5Ctimes\%202} and a
\includegraphics{https://latex.codecogs.com/png.latex?2\%20\%5Ctimes\%203} to
make a
\includegraphics{https://latex.codecogs.com/png.latex?4\%20\%5Ctimes\%203} ?

We \emph{can} compute
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BC\%7D} using
only the encodings
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D} and
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BB\%7D}! We call
this \textbf{matrix multiplication}. It's typically denoted as
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BC\%7D\%20\%3D\%20\%5Chat\%7BA\%7D\%20\%5Chat\%7BB\%7D}.

That's exactly what \emph{matrix multiplication} is defined as. If:

\begin{itemize}
\tightlist
\item
  \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D} is a
  \includegraphics{https://latex.codecogs.com/png.latex?O\%20\%5Ctimes\%20M}
  matrix representing a linear transformation from a M-dimensional space to an
  O-dimensional space
\item
  \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BB\%7D} is an
  \includegraphics{https://latex.codecogs.com/png.latex?M\%20\%5Ctimes\%20N}
  matrix representing a linear transformation from an N-dimensional space to an
  M-dimensional space
\end{itemize}

Then:

\begin{itemize}
\tightlist
\item
  \includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BC\%7D\%20\%3D\%20\%5Chat\%7BA\%7D\%5Chat\%7BB\%7D}
  is a
  \includegraphics{https://latex.codecogs.com/png.latex?O\%20\%5Ctimes\%20N}
  matrix representing a linear transformation from an N-dimensional space to an
  O-dimensional space.
\end{itemize}

Again -- manipulation of our \emph{encodings} can manifest the manipulation in
the \emph{linear transformations} that we want.

Symbolically, if we treat function application as matrix-vector multiplication,
this means that
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D\%5Chat\%7BB\%7D}
is defined such that
\includegraphics{https://latex.codecogs.com/png.latex?\%28\%5Chat\%7BA\%7D\%5Chat\%7BB\%7D\%29\%5Cmathbf\%7Bx\%7D\%20\%3D\%20\%5Chat\%7BA\%7D\%28\%5Chat\%7BB\%7D\%5Cmathbf\%7Bx\%7D\%29}.

In that notation, it kinda looks like the associativity of multiplication,
doesn't it? Don't be fooled!
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BA\%7D\%20\%5Chat\%7BB\%7D},
matrix-matrix multiplication, is a completely different type of operation than
\includegraphics{https://latex.codecogs.com/png.latex?\%5Chat\%7BB\%7D\%5Cmathbf\%7Bv\%7D}.
One is the symbolic manipulation on \emph{two encodings} of of a linear
transformation, and the other is an \emph{application} of an encoding of a
linear transformation on encoding of a vector.

If you're familiar with Haskell idioms, matrix-matrix multiplication is like
\texttt{.} (function composition), and matrix-vector multiplication is like
\texttt{\$}, or function application. One is a ``higher order function'': taking
two functions (at least, the encodings of them) and returning a new function.
The other is an application of a function to its input.

And, like in Haskell:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(f }\OperatorTok{.}\NormalTok{ g) x }\OtherTok{=}\NormalTok{ f (g x)}
\end{Highlighting}
\end{Shaded}

We won't go over the actual process of computing the matrix-matrix product, but
it's something that you can work out just in terms of the definitions of the
encodings. Just manually apply out everything and group together common factors
of the basis vectors of the destination space.

\hypertarget{the-big-picture}{%
\section{The Big Picture}\label{the-big-picture}}

At the highest level here, what we're doing is taking a \emph{function} and
encoding it as \emph{data} -- a parameterization of that function. Essentially,
we take the properties of the type of functions we are looking at and find out
that it can be defined/represented in a limited number of parameters

Then, the breakthrough is that we look at useful higher-order functions and
manipulations of those transformations. Then, we see how we can implement those
\emph{transformations} by symbolically manipulating the \emph{encodings}!

This is actually a dance we do all the time in programming. Instead of working
with functions, we work with reified data that represent those functions. And,
instead of direct higher order functions, we transform that data in a way that
makes it encodes the function we want to produce.

Matrices are exactly that. Linear transformations are the functions we want to
analyze, and we realize that we can completely specify/define any linear
transformation with a matrix (against a choice of bases).

Then, we realize that there are some nice manipulations we can do on linear
transformations; we can combine them to create new linear transformations in
useful ways.

However, because those manipulations all produce \emph{new} linear
transformations, we know that their results can all be encoded in \emph{new}
matrices. So, we see if we can just directly apply those manipulations by
directly working on those matrices!

I hope this post serves to demystify matrices, matrix addition, and
multiplication for you, and help you see why they are defined the way that they
are. Furthermore, I hope that it gives some insight on why matrices are useful
in linear algebra, and also how similar encodings can help you with manipulating
other types of functions!

\hypertarget{signoff}{%
\section{Signoff}\label{signoff}}

Hi, thanks for reading! You can reach me via email at
\href{mailto:justin@jle.im}{\nolinkurl{justin@jle.im}}, or at twitter at
\href{https://twitter.com/mstk}{@mstk}! This post and all others are published
under the \href{https://creativecommons.org/licenses/by-nc-nd/3.0/}{CC-BY-NC-ND
3.0} license. Corrections and edits via pull request are welcome and encouraged
at \href{https://github.com/mstksg/inCode}{the source repository}.

If you feel inclined, or this post was particularly helpful for you, why not
consider \href{https://www.patreon.com/justinle/overview}{supporting me on
Patreon}, or a \href{bitcoin:3D7rmAYgbDnp4gp4rf22THsGt74fNucPDU}{BTC donation}?
:)

\end{document}
